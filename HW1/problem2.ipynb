{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBLEM 2 : Kosarak Association Rules\n",
    "Your task is to take a dataset of nearly one million clicks on a news site16 and use the Weka Explorer to identify interesting association rules. \n",
    "Ordinarily this would be a point-and-click task; \n",
    "however, the input data format is a list of transactions (each line in the file includes a list of anonymized news item id’s), \n",
    "whereas Weka requires a tabular format. \n",
    "Specifically, each distinct news item id should be represented via a column/attribute, \n",
    "and each row/instance should be a sequence of binary values, indicating whether or not the user visited the corresponding news item.\n",
    "\n",
    "A. Write a Python program which takes as its argument5 the path to a text file of data (assumed to be in the itemset format above) and produces as output to the console a sparse ARFF file.\n",
    "# numbers need to be sorted\n",
    "B. Use your program to convert the kosarak.dat file to a sparse kosarak.arff. About how long did it take to run?\n",
    "C. Load the resulting file into Weka (as described above; you should have 41,270 attributes and 990, 002 instances). About how long did it take to load this file?\n",
    "\n",
    "D. Use Weka’s FP-Growth implementation to find rules that have support count of at least 49, 500 and confidence of at least 99% – record your rules (there should be 2).\n",
    "E. Run the algorithm at least 5 times. Then look to the log and record how much time each took. How does the average time compare to the time necessary to convert the dataset and then load into Weka?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: liac-arff in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#A\n",
    "%pip install liac-arff\n",
    "import arff\n",
    "%pip install os\n",
    "import os\n",
    "%pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 990002/990002 [53:59<00:00, 305.62it/s]  \n"
     ]
    }
   ],
   "source": [
    "data_file = \"kosarak.dat\"\n",
    "data_file_path = os.path.join(os.getcwd(), data_file)\n",
    "data = []\n",
    "unique_items = set()\n",
    "with open(data_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        items = line.split()\n",
    "        items = [int(item) for item in items]\n",
    "        items.sort()\n",
    "        data.append(items)\n",
    "        \n",
    "        #check each item and ensure it is unique, if not add it to the unique_items set\n",
    "        for item in items:\n",
    "            if item not in unique_items:\n",
    "                unique_items.add(item)\n",
    "\n",
    "\n",
    "\n",
    "# Generate attributes based on unique items\n",
    "# attributes = [(item, 'NUMERIC') for item in sorted(unique_items)]\n",
    "\n",
    "# arff.dump('kosarak.arff', data, relation='kosarak', names=attributes)\n",
    "\n",
    "with open('kosarak.arff', 'w') as f:\n",
    "    f.write('@RELATION kosarak\\n')\n",
    "    for item in sorted(unique_items):\n",
    "        f.write(f'@ATTRIBUTE {item} {{0,1}}\\n') #maybe comma \n",
    "    f.write('\\n')\n",
    "    f.write('@DATA\\n')\n",
    "    # write data\n",
    "    for items in tqdm(data):\n",
    "        items = [int(item) for item in items]\n",
    "        f.write('{')\n",
    "        f.write(','.join([f'{item-1} 1' for item in sorted(unique_items) if item in items]))\n",
    "        f.write('}\\n')\n",
    "        # f.write(','.join(items) + '\\n')\n",
    "#output: Sparse ARFF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBSERVATIONS & RESPONSES\n",
    "\n",
    "## B. Use your program to convert the kosarak.dat file to a sparse kosarak.arff. About how long did it take to run?\n",
    "\n",
    "Using tqdm, it took 54 minutes to complete. \n",
    "\n",
    "## C. Load the resulting file into Weka (as described above; you should have 41,270 attributes and 990, 002 instances). About how long did it take to load this file?\n",
    "\n",
    "Not that long, maybe about 3 seconds, if that. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Use Weka’s FP-Growth implementation to find rules that have support count of at least 49, 500 and confidence of at least 99% – record your rules (there should be 2).\n",
    "\n",
    "=== Run information ===\n",
    "\n",
    "Scheme:       weka.associations.FPGrowth -P 2 -I -1 -N 2 -T 0 -C 0.99 -D 0.05 -U 1.0 -M 49500.0 -S\n",
    "\n",
    "Relation:     kosarak\n",
    "\n",
    "Instances:    990002\n",
    "\n",
    "Attributes:   41270\n",
    "\n",
    "[list of attributes omitted]\n",
    "\n",
    "=== Associator model (full training set) ===\n",
    "\n",
    "\n",
    "FPGrowth found 2 rules\n",
    "\n",
    "1. [11=1, 218=1, 148=1]: 50098 ==> [6=1]: 49866   <conf:(1)> lift:(1.64) lev:(0.02) conv:(84.4) \n",
    "\n",
    "2. [11=1, 148=1]: 55759 ==> [6=1]: 55230   <conf:(0.99)> lift:(1.63) lev:(0.02) conv:(41.3) \n",
    "\n",
    "\n",
    "## Using the wiki's documentation to interpret these results\n",
    "\n",
    "\"the number before the arrow is the number of instances for which the antecedent is true;\n",
    "that after the arrow is the number of instances for which the consequent is true also; \n",
    "and the confidence (in parentheses) is the ratio between the two.\"\n",
    "\n",
    "These rules state:\n",
    "1. There is a relationship between the transactions of items [11, 218, 148] being purchased/made 50098 times, to have item [6] be bought 49866 times. The conf of (1) states that this association has a ratio of 1.0 = 100% (49866 / 50098 = 0.995 -> rounded)\n",
    "\n",
    "2. Ratio of items [11, 148] : [6] = 55230 / 55759 = 0.99\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Run the algorithm at least 5 times. Then look to the log and record how much time each took. How does the average time compare to the time necessary to convert the dataset and then load into Weka?\n",
    "\n",
    "Out of 6 runs:\n",
    "\n",
    "3 runs took 1 second between start - finish. \n",
    "\n",
    "3 runs were performed within/less than a second (start-finish = 0)\n",
    "\n",
    "The time to convert the dataset is long at 45-50 minutes. Loading into Weka thereafter took only a few seconds. Then running the FP Growth algorithm took less than a second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
