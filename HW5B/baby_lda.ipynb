{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fa538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow pseudo-code:\n",
    "\n",
    "# clean text for stopwords, frequent words, short words, numerical words etc.\n",
    "#N docs, K topics, W words, length(d)=# of words in doc d, DLMAX= max doc length\n",
    "#N x DLMAX matrix DOCS[d,i]=w : i-th word in doc d is w (w is an index in Vocab dictionary)\n",
    "# alternatively to DOCS one can use Dx W matrix X[d,w] = count of word w in doc d\n",
    "# Wx1 array Vocab[w] = actual word string\n",
    "\n",
    "# set number of topics >\n",
    "K=6\n",
    "\n",
    "# N x K matrix A[d,k] : count of times topic k is sampled for doc d\n",
    "# K x W matrix B[k,w] : count of times topic k is sampled for word w\n",
    "\n",
    "# 1 x K array alpha : dirichlet uniform prior of doc over topics; below 5-1=4 is the strength of the prior\n",
    "alpha = 5*ones(1,K)\n",
    "\n",
    "# 1 x W array beta: dirichlet unif prior of topic over words; 2-1=1 is the strength of the prior\n",
    "beta = 2*ones(1, W)\n",
    "\n",
    "# N x DLMAX matrix Z[d,i] = k ; topic k currently sampled for i-th word in doc d; initially zero = no topic\n",
    "Z=zeros(D,DLMAX)\n",
    "\n",
    "# start A as alpha prior for each doc (each row is alpha)\n",
    "A = repmat (alpha, N, 1)\n",
    "\n",
    "# start B as beta prior for each topic (each row is beta)\n",
    "B = repmat (beta, K, 1)\n",
    "\n",
    "# K x 1 array BSUM = sum of B over all words, per topic\n",
    "BSUM = sum(B,2)\n",
    "\n",
    "#resample topic for each word in each doc, T=1000 iterations\n",
    "for each iteration T=1:1000\n",
    "for each doc d=1:D\n",
    "for each index i=1:length(d)\n",
    "   w = DOCD(d,i) # the word\n",
    "   zi = Z(d,i) # current topic\n",
    "\n",
    "   # subtract current topic zi from counts\n",
    "   if (zi>0)\n",
    "     A(d,zi) = A(d,zi) -1\n",
    "     B(zi,w) = B(zi,w) -1\n",
    "     BSUM(zi) = BSUM(zi) -1\n",
    "   end_if\n",
    "\n",
    "   #prepare Gibbs-sampling cond distribution over topics prob(zi=k | all-else-sampled)\n",
    "   #dist (unnormalized) is A(d_row) .* B(k_column)-normalized-k\n",
    "   \".*\" is product for each component, example [2 3 -1] .* [2 8 2] = [ 4 24 -2]\n",
    "   dst = A(d,:) .* (B(:,w) ./ BSUM)'\n",
    "   # this calculation corresponds to the derivation below( LDA simplified ), where n_dk is our A and n_kw is our B\n",
    "  \n",
    "\n",
    "   #sample a new topic from nonuniform discrete dst over topics ;\n",
    "   #you can use a built-in function or do binary-search over cdf(dst)\n",
    "   new_zi = randsample(dst)\n",
    "   #update Z and counts\n",
    "   Z(d,i)= new_zi\n",
    "   A(d,new_zi) = A(d,new_zi) +1\n",
    "   B(new_zi,w) = B(new_zi,w) + 1\n",
    "   BSUM(new_zi) = BSUM(new_zi) +1\n",
    "end_for_loops\n",
    "\n",
    "# each row in A is the doc d distribution over topics (unnormalized)\n",
    "# each row in B is the topic k distribution words (unnormalized)\n",
    "\n",
    "# display a \"wordcloud\" for each topic, using B for word weights\n",
    "for k=1:K\n",
    "   figure(k); clf;\n",
    "   wordcloud(Vocab, B(k,:));\n",
    "end_for"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
