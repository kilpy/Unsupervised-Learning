{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 1: Topic Models\n",
    "## Obtain Topic Models (K=10, 20, 50) for both datasets by running LDA and NMF methods; you can call libraries for both methods and dont have to use the ES index as source. For both LDA and NMF: print out for each topic the top 20 words (with probabilities)\n",
    "\n",
    "The rest of of topic exercises and results are required only for the LDA topics:\n",
    "\n",
    "- 20NG: how well the topics align with the 20NG label classes? This is not asking for a measurement, but rather for a visual inspection to determine what topics match well with what classes. \n",
    "\n",
    "Does this change if one increases the topics from 20 to 50?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "well folks, my mac plus finally gave up the ghost this weekend after\n",
      "starting life as a 512k way back in 1985.  sooo, i'm in the market for a\n",
      "new machine a bit sooner than i intended to be...\n",
      "\n",
      "i'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\n",
      "of questions that (hopefully) somebody can answer:\n",
      "\n",
      "* does anybody know any dirt on when the next round of powerbook\n",
      "introductions are expected?  i'd heard the 185c was supposed to make an\n",
      "appearence \"this summer\" but haven't heard anymore on it - and since i\n",
      "don't have access to macleak, i was wondering if anybody out there had\n",
      "more info...\n",
      "\n",
      "* has anybody heard rumors about price drops to the powerbook line like the\n",
      "ones the duo's just went through recently?\n",
      "\n",
      "* what's the impression of the display on the 180?  i could probably swing\n",
      "a 180 if i got the 80Mb disk rather than the 120, but i don't really have\n",
      "a feel for how much \"better\" the display is (yea, it looks great in the\n",
      "store, but is that all \"wow\" or is it really that good?).  could i solicit\n",
      "some opinions of people who use the 160 and 180 day-to-day on if its worth\n",
      "taking the disk size and money hit to get the active display?  (i realize\n",
      "this is a real subjective question, but i've only played around with the\n",
      "machines in a computer store breifly and figured the opinions of somebody\n",
      "who actually uses the machine daily might prove helpful).\n",
      "\n",
      "* how well does hellcats perform?  ;)\n",
      "\n",
      "thanks a bunch in advance for any info - if you could email, i'll post a\n",
      "summary (news reading time is at a premium with finals just around the\n",
      "corner... :( )\n",
      "--\n",
      "Tom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering\n"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html#sphx-glr-download-auto-examples-tutorials-run-lda-py\n",
    "\n",
    "# imports\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from gensim import corpora, models, similarities, downloader\n",
    "\n",
    "# Load the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "# truncate the dataset to 1000 documents for faster processing\n",
    "# newsgroups.data = newsgroups.data[:1000]\n",
    "\n",
    "print(type(newsgroups.data[0][:]))  # <class 'scipy.sparse.csr.csr_matrix'>\n",
    "print(newsgroups.data[2][:])  # <class 'numpy.ndarray'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'a', '512k', 'way', 'back', 'in', '1985', 'sooo', 'i', 'm', 'in', 'the', 'market', 'for', 'a', 'new', 'machine', 'a', 'bit', 'sooner', 'than', 'i', 'intended', 'to', 'be', 'i', 'm', 'looking', 'into', 'picking', 'up', 'a', 'powerbook', '160', 'or', 'maybe', '180', 'and', 'have', 'a', 'bunch', 'of', 'questions', 'that', 'hopefully', 'somebody', 'can', 'answer', 'does', 'anybody', 'know', 'any', 'dirt', 'on', 'when', 'the', 'next', 'round', 'of', 'powerbook', 'introductions', 'are', 'expected', 'i', 'd', 'heard', 'the', '185c', 'was', 'supposed', 'to', 'make', 'an', 'appearence', 'this', 'summer', 'but', 'haven', 't', 'heard', 'anymore', 'on', 'it', 'and', 'since', 'i', 'don', 't', 'have', 'access', 'to', 'macleak', 'i', 'was', 'wondering', 'if', 'anybody', 'out', 'there', 'had', 'more', 'info', 'has', 'anybody', 'heard', 'rumors', 'about', 'price', 'drops', 'to', 'the', 'powerbook', 'line', 'like', 'the', 'ones', 'the', 'duo', 's', 'just', 'went', 'through', 'recently', 'what', 's', 'the', 'impression', 'of', 'the', 'display', 'on', 'the', '180', 'i', 'could', 'probably', 'swing', 'a', '180', 'if', 'i', 'got', 'the', '80mb', 'disk', 'rather', 'than', 'the', '120', 'but', 'i', 'don', 't', 'really', 'have', 'a', 'feel', 'for', 'how', 'much', 'better', 'the', 'display', 'is', 'yea', 'it', 'looks', 'great', 'in', 'the', 'store', 'but', 'is', 'that', 'all', 'wow', 'or', 'is', 'it', 'really', 'that', 'good', 'could', 'i', 'solicit', 'some', 'opinions', 'of', 'people', 'who', 'use', 'the', '160', 'and', '180', 'day', 'to', 'day', 'on', 'if', 'its', 'worth', 'taking', 'the', 'disk', 'size', 'and', 'money', 'hit', 'to', 'get', 'the', 'active', 'display', 'i', 'realize', 'this', 'is', 'a', 'real', 'subjective', 'question', 'but', 'i', 've', 'only', 'played', 'around', 'with', 'the', 'machines', 'in', 'a', 'computer', 'store', 'breifly', 'and', 'figured', 'the', 'opinions', 'of', 'somebody', 'who', 'actually', 'uses', 'the', 'machine', 'daily', 'might', 'prove', 'helpful', 'how', 'well', 'does', 'hellcats', 'perform', 'thanks', 'a', 'bunch', 'in', 'advance', 'for', 'any', 'info', 'if', 'you', 'could', 'email', 'i', 'll', 'post', 'a', 'summary', 'news', 'reading', 'time', 'is', 'at', 'a', 'premium', 'with', 'finals', 'just', 'around', 'the', 'corner', 'tom', 'willis', 'twillis', 'ecn', 'purdue', 'edu', 'purdue', 'electrical', 'engineering']\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "# gensim expects a list of lists of tokens, so we need to convert the sparse matrix to a list of lists\n",
    "# %pip install nltk\n",
    "# Tokenize the documents\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for idx in range(len(newsgroups.data)):\n",
    "    newsgroups.data[idx] = newsgroups.data[idx][:].lower()  # convert to lowercase\n",
    "    newsgroups.data[idx] = tokenizer.tokenize(newsgroups.data[idx])  # split into words\n",
    "\n",
    "print(newsgroups.data[2]) \n",
    "print(len(newsgroups.data[2])) # number of tokens in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'folks', 'my', 'mac', 'plus', 'finally', 'gave', 'up', 'the', 'ghost', 'this', 'weekend', 'after', 'starting', 'life', 'as', 'a', '512k', 'way', 'back', 'in', '1985', 'sooo', 'i', 'm', 'in', 'the', 'market', 'for', 'a', 'new', 'machine', 'a', 'bit', 'sooner', 'than', 'i', 'intended', 'to', 'be', 'i', 'm', 'looking', 'into', 'picking', 'up', 'a', 'powerbook', '160', 'or', 'maybe', '180', 'and', 'have', 'a', 'bunch', 'of', 'questions', 'that', 'hopefully', 'somebody', 'can', 'answer', 'does', 'anybody', 'know', 'any', 'dirt', 'on', 'when', 'the', 'next', 'round', 'of', 'powerbook', 'introductions', 'are', 'expected', 'i', 'd', 'heard', 'the', '185c', 'was', 'supposed', 'to', 'make', 'an', 'appearence', 'this', 'summer', 'but', 'haven', 't', 'heard', 'anymore', 'on', 'it', 'and', 'since', 'i', 'don', 't', 'have', 'access', 'to', 'macleak', 'i', 'was', 'wondering', 'if', 'anybody', 'out', 'there', 'had', 'more', 'info', 'has', 'anybody', 'heard', 'rumors', 'about', 'price', 'drops', 'to', 'the', 'powerbook', 'line', 'like', 'the', 'ones', 'the', 'duo', 's', 'just', 'went', 'through', 'recently', 'what', 's', 'the', 'impression', 'of', 'the', 'display', 'on', 'the', '180', 'i', 'could', 'probably', 'swing', 'a', '180', 'if', 'i', 'got', 'the', '80mb', 'disk', 'rather', 'than', 'the', '120', 'but', 'i', 'don', 't', 'really', 'have', 'a', 'feel', 'for', 'how', 'much', 'better', 'the', 'display', 'is', 'yea', 'it', 'looks', 'great', 'in', 'the', 'store', 'but', 'is', 'that', 'all', 'wow', 'or', 'is', 'it', 'really', 'that', 'good', 'could', 'i', 'solicit', 'some', 'opinions', 'of', 'people', 'who', 'use', 'the', '160', 'and', '180', 'day', 'to', 'day', 'on', 'if', 'its', 'worth', 'taking', 'the', 'disk', 'size', 'and', 'money', 'hit', 'to', 'get', 'the', 'active', 'display', 'i', 'realize', 'this', 'is', 'a', 'real', 'subjective', 'question', 'but', 'i', 've', 'only', 'played', 'around', 'with', 'the', 'machines', 'in', 'a', 'computer', 'store', 'breifly', 'and', 'figured', 'the', 'opinions', 'of', 'somebody', 'who', 'actually', 'uses', 'the', 'machine', 'daily', 'might', 'prove', 'helpful', 'how', 'well', 'does', 'hellcats', 'perform', 'thanks', 'a', 'bunch', 'in', 'advance', 'for', 'any', 'info', 'if', 'you', 'could', 'email', 'i', 'll', 'post', 'a', 'summary', 'news', 'reading', 'time', 'is', 'at', 'a', 'premium', 'with', 'finals', 'just', 'around', 'the', 'corner', 'tom', 'willis', 'twillis', 'ecn', 'purdue', 'edu', 'purdue', 'electrical', 'engineering']\n",
      "310\n"
     ]
    }
   ],
   "source": [
    "# Remove numbers, but not words that contain numbers.\n",
    "for ng in newsgroups.data:\n",
    "    ng = [token for token in ng if not token.isdigit()]\n",
    "    # Remove words that are only one character.\n",
    "    ng = [token for token in ng if len(token) > 1]\n",
    "\n",
    "\n",
    "\n",
    "print(newsgroups.data[2])  \n",
    "print(len(newsgroups.data[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\koola\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\koola\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "newsgroups.data = [[lemmatizer.lemmatize(token) for token in doc] for doc in newsgroups.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams\n",
    "from gensim.models import Phrases\n",
    "bigrams = Phrases(newsgroups.data, min_count=5, threshold=100)\n",
    "# bigrams = Phrases(newsgroups.data, min_count=20)\n",
    "for idx in range(len(newsgroups.data)):\n",
    "    for token in bigrams[newsgroups.data[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            newsgroups.data[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words that appear in less than 20 docs OR in more than 50% of the docs.\n",
    "\n",
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(newsgroups.data)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "# dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 98832\n",
      "Number of documents: 11314\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in newsgroups.data]\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are ready to train the LDA model. We will first discuss how to set some of the training parameters.\n",
    "\n",
    "# chunksize \n",
    "    # controls how many documents are processed at a time in the training algorithm. \n",
    "    # Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory. \n",
    "    # I’ve set chunksize = 2000, which is more than the amount of documents, so I process all the data in one go. \n",
    "    # Chunksize can however influence the quality of the model, as discussed in Hoffman and co-authors [2], but the difference was not substantial in this case.\n",
    "# passes\n",
    "    # controls how often we train the model on the entire corpus. \n",
    "    # Another word for passes might be “epochs”. iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document.\n",
    "    # It is important to set the number of “passes” and “iterations” high enough.\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 20\n",
    "chunksize = 1000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "# Train the model.\n",
    "model = LdaModel(corpus=corpus, \n",
    "                 id2word=id2word, \n",
    "                 chunksize=chunksize, \n",
    "                 passes=passes, \n",
    "                 iterations=iterations, \n",
    "                 num_topics=num_topics, \n",
    "                 eval_every=eval_every)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.053*\"the\" + 0.037*\"of\" + 0.031*\"to\" + 0.027*\"that\"')\n",
      "(1, '0.018*\"france\" + 0.012*\"ryan\" + 0.007*\"bathroom\" + 0.006*\"bedroom\"')\n",
      "(2, '0.034*\"please\" + 0.033*\"mail\" + 0.027*\"for\" + 0.023*\"thanks\"')\n",
      "(3, '0.521*\"ax\" + 0.038*\"max\" + 0.028*\"q\" + 0.022*\"3\"')\n",
      "(4, '0.055*\"m\" + 0.042*\"_\" + 0.034*\"w\" + 0.023*\"1\"')\n",
      "(5, '0.022*\"cd\" + 0.018*\"adam\" + 0.013*\"ctrl\" + 0.009*\"da\"')\n",
      "(6, '0.040*\"the\" + 0.025*\"and\" + 0.021*\"a\" + 0.018*\"to\"')\n",
      "(7, '0.003*\"uoknor\" + 0.003*\"uoknor_edu\" + 0.000*\"dash\" + 0.000*\"seningen\"')\n",
      "(8, '0.089*\"1\" + 0.077*\"0\" + 0.057*\"2\" + 0.042*\"3\"')\n",
      "(9, '0.004*\"ethernet_card\" + 0.004*\"toshiba\" + 0.002*\"expansion_slot\" + 0.002*\"internal_modem\"')\n",
      "(10, '0.024*\"space\" + 0.014*\"edu\" + 0.008*\"nasa\" + 0.008*\"at\"')\n",
      "(11, '0.054*\"the\" + 0.040*\"a\" + 0.035*\"i\" + 0.033*\"to\"')\n",
      "(12, '0.035*\"0\" + 0.033*\"game\" + 0.032*\"team\" + 0.017*\"player\"')\n",
      "(13, '0.026*\"rider\" + 0.015*\"suck\" + 0.011*\"beach\" + 0.006*\"lciii\"')\n",
      "(14, '0.019*\"char\" + 0.009*\"coast\" + 0.007*\"apr_1993\" + 0.006*\"mark\"')\n",
      "(15, '0.009*\"compuserve\" + 0.004*\"compuserve_com\" + 0.003*\"832\" + 0.002*\"ponder\"')\n",
      "(16, '0.087*\"the\" + 0.048*\"of\" + 0.033*\"and\" + 0.029*\"in\"')\n",
      "(17, '0.046*\"the\" + 0.043*\"he\" + 0.038*\"wa\" + 0.034*\"and\"')\n",
      "(18, '0.024*\"bank\" + 0.016*\"pitt\" + 0.014*\"patient\" + 0.014*\"gordon\"')\n",
      "(19, '0.325*\"x\" + 0.035*\"entry\" + 0.028*\"n\" + 0.013*\"client\"')\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the topics\n",
    "topics = model.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)  # topic number, topic words, topic probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -5.4757.\n",
      "[([(0.05397378, 'the'),\n",
      "   (0.039999828, 'a'),\n",
      "   (0.035221323, 'i'),\n",
      "   (0.032537278, 'to'),\n",
      "   (0.021373285, 'it'),\n",
      "   (0.018254673, 'is'),\n",
      "   (0.017383188, 'and'),\n",
      "   (0.017112624, 'that'),\n",
      "   (0.01631468, 'you'),\n",
      "   (0.015336333, 'of'),\n",
      "   (0.012827275, 'in'),\n",
      "   (0.011845945, 'have'),\n",
      "   (0.0111008305, 'for'),\n",
      "   (0.010212261, 'this'),\n",
      "   (0.010098661, 't'),\n",
      "   (0.009882227, 'be'),\n",
      "   (0.00927743, 'if'),\n",
      "   (0.008976932, 'on'),\n",
      "   (0.008131061, 's'),\n",
      "   (0.007927541, 'but')],\n",
      "  -0.5332465596989693),\n",
      " ([(0.052510083, 'the'),\n",
      "   (0.037010893, 'of'),\n",
      "   (0.030909758, 'to'),\n",
      "   (0.027391419, 'that'),\n",
      "   (0.027091673, 'a'),\n",
      "   (0.026754776, 'is'),\n",
      "   (0.020334795, 'and'),\n",
      "   (0.017405462, 'in'),\n",
      "   (0.01518277, 'it'),\n",
      "   (0.015003293, 'not'),\n",
      "   (0.013101078, 'you'),\n",
      "   (0.011706672, 'are'),\n",
      "   (0.009435979, 'i'),\n",
      "   (0.009406719, 'this'),\n",
      "   (0.00922601, 'be'),\n",
      "   (0.007360744, 'for'),\n",
      "   (0.0069384393, 'have'),\n",
      "   (0.006410113, 'we'),\n",
      "   (0.0061169285, 'or'),\n",
      "   (0.006029382, 'but')],\n",
      "  -0.5885072615159995),\n",
      " ([(0.087036215, 'the'),\n",
      "   (0.04843227, 'of'),\n",
      "   (0.033024397, 'and'),\n",
      "   (0.02870453, 'in'),\n",
      "   (0.026577437, 'to'),\n",
      "   (0.021588562, 'a'),\n",
      "   (0.012212842, 'for'),\n",
      "   (0.009804182, 'by'),\n",
      "   (0.006505063, 'on'),\n",
      "   (0.0062635774, 'from'),\n",
      "   (0.0058408817, 'with'),\n",
      "   (0.00558101, 's'),\n",
      "   (0.0051964074, 'be'),\n",
      "   (0.0048515443, 'are'),\n",
      "   (0.004776638, 'is'),\n",
      "   (0.0046547577, 'will'),\n",
      "   (0.0044970787, 'that'),\n",
      "   (0.0044317404, 'their'),\n",
      "   (0.0041545522, 'this'),\n",
      "   (0.0040585217, 'state')],\n",
      "  -0.826110374597775),\n",
      " ([(0.045734458, 'the'),\n",
      "   (0.042643856, 'he'),\n",
      "   (0.037830316, 'wa'),\n",
      "   (0.03351903, 'and'),\n",
      "   (0.024061294, 'i'),\n",
      "   (0.022367736, 'they'),\n",
      "   (0.017469706, 'to'),\n",
      "   (0.017463917, 'that'),\n",
      "   (0.017125836, 'in'),\n",
      "   (0.016203549, 'his'),\n",
      "   (0.015464608, 'had'),\n",
      "   (0.014625703, 'were'),\n",
      "   (0.0126109235, 't'),\n",
      "   (0.011330536, 'we'),\n",
      "   (0.01124614, 's'),\n",
      "   (0.009497409, 'it'),\n",
      "   (0.009401331, 'him'),\n",
      "   (0.00905286, 'on'),\n",
      "   (0.008741684, 'said'),\n",
      "   (0.00845638, 'at')],\n",
      "  -0.9754901950408943),\n",
      " ([(0.039648697, 'the'),\n",
      "   (0.02539061, 'and'),\n",
      "   (0.020526733, 'a'),\n",
      "   (0.017917354, 'to'),\n",
      "   (0.017341938, 'for'),\n",
      "   (0.016557021, 'is'),\n",
      "   (0.016101088, 'of'),\n",
      "   (0.01317682, 'in'),\n",
      "   (0.009688692, 'with'),\n",
      "   (0.009560283, 'on'),\n",
      "   (0.008920544, '1'),\n",
      "   (0.00858827, 'or'),\n",
      "   (0.0078121293, 'file'),\n",
      "   (0.007577297, 'drive'),\n",
      "   (0.007454045, 'window'),\n",
      "   (0.0072449227, 'from'),\n",
      "   (0.006623596, 'are'),\n",
      "   (0.006504526, '2'),\n",
      "   (0.0062294933, 'system'),\n",
      "   (0.005850275, 'use')],\n",
      "  -1.3223265774799475),\n",
      " ([(0.003129029, 'uoknor_edu'),\n",
      "   (0.003129029, 'uoknor'),\n",
      "   (1.0055122e-05, 'seningen'),\n",
      "   (1.0055122e-05, 'dash'),\n",
      "   (1.0055092e-05, '85mph'),\n",
      "   (1.0055092e-05, 'constellation'),\n",
      "   (1.0055092e-05, 'girlfriend'),\n",
      "   (1.0055092e-05, 'musta'),\n",
      "   (1.0055092e-05, 'speedo'),\n",
      "   (1.0055092e-05, '187kph'),\n",
      "   (1.0055092e-05, 'novakovic'),\n",
      "   (1.0055092e-05, 'maserati'),\n",
      "   (1.0055092e-05, 'hehe'),\n",
      "   (1.0055092e-05, '302'),\n",
      "   (1.0055092e-05, 'coulda'),\n",
      "   (1.0055092e-05, '116mph'),\n",
      "   (1.0055092e-05, 'anemic'),\n",
      "   (1.0055092e-05, 'toyed'),\n",
      "   (1.0055092e-05, 'kilometer'),\n",
      "   (1.0055092e-05, 'turbocoupe')],\n",
      "  -1.3838140313096627),\n",
      " ([(0.08864657, '1'),\n",
      "   (0.077250496, '0'),\n",
      "   (0.056893915, '2'),\n",
      "   (0.04195871, '3'),\n",
      "   (0.03511864, '4'),\n",
      "   (0.031814907, '5'),\n",
      "   (0.028777681, '25'),\n",
      "   (0.023922388, '6'),\n",
      "   (0.022974841, '7'),\n",
      "   (0.018403405, '8'),\n",
      "   (0.01699259, '10'),\n",
      "   (0.015567911, '9'),\n",
      "   (0.013203007, '00'),\n",
      "   (0.011777571, '16'),\n",
      "   (0.011255382, '15'),\n",
      "   (0.009915441, '20'),\n",
      "   (0.009860561, '11'),\n",
      "   (0.009285064, '12'),\n",
      "   (0.008726218, '14'),\n",
      "   (0.008358798, '17')],\n",
      "  -1.424676482262454),\n",
      " ([(0.52052134, 'ax'),\n",
      "   (0.03837488, 'max'),\n",
      "   (0.028344555, 'q'),\n",
      "   (0.021608563, '3'),\n",
      "   (0.0128884, 'g9v'),\n",
      "   (0.010918278, 'p'),\n",
      "   (0.010302532, '7'),\n",
      "   (0.009310653, 'r'),\n",
      "   (0.008999157, 'm'),\n",
      "   (0.0084765125, 'g'),\n",
      "   (0.007402602, 'n'),\n",
      "   (0.0071187117, 'b8f'),\n",
      "   (0.0062533426, 'pl'),\n",
      "   (0.0059173456, 'a86'),\n",
      "   (0.005730805, '_'),\n",
      "   (0.0052306172, 'g9v_g9v'),\n",
      "   (0.0045822114, '2'),\n",
      "   (0.0045685717, '1d9'),\n",
      "   (0.004447331, '9'),\n",
      "   (0.004403916, 'a')],\n",
      "  -1.5239971690266396),\n",
      " ([(0.054875072, 'm'),\n",
      "   (0.04208192, '_'),\n",
      "   (0.03389079, 'w'),\n",
      "   (0.023440156, '1'),\n",
      "   (0.022917757, 's'),\n",
      "   (0.020400196, 'r'),\n",
      "   (0.020147309, 'g'),\n",
      "   (0.01965194, 'p'),\n",
      "   (0.019238057, 'v'),\n",
      "   (0.017483145, 'c'),\n",
      "   (0.017345296, 'o'),\n",
      "   (0.01731326, 'k'),\n",
      "   (0.017204233, 't'),\n",
      "   (0.017097149, 'i'),\n",
      "   (0.01695872, 'u'),\n",
      "   (0.016795723, 'b'),\n",
      "   (0.015841812, 'z'),\n",
      "   (0.015785497, '0'),\n",
      "   (0.015507792, '6'),\n",
      "   (0.01509219, '8')],\n",
      "  -1.5500939324418848),\n",
      " ([(0.035359833, '0'),\n",
      "   (0.03254061, 'game'),\n",
      "   (0.03219376, 'team'),\n",
      "   (0.01675498, 'player'),\n",
      "   (0.014143189, 'hockey'),\n",
      "   (0.013370498, 'season'),\n",
      "   (0.01087448, 'league'),\n",
      "   (0.00995783, '36'),\n",
      "   (0.0079633035, 'play'),\n",
      "   (0.0073448964, 'year'),\n",
      "   (0.007166662, '80'),\n",
      "   (0.006260843, '000'),\n",
      "   (0.0059807957, 'fan'),\n",
      "   (0.005614349, '60'),\n",
      "   (0.005142044, '70'),\n",
      "   (0.004992535, 'division'),\n",
      "   (0.004833219, 's'),\n",
      "   (0.004718013, 'baseball'),\n",
      "   (0.0046678367, '65'),\n",
      "   (0.004587756, '92')],\n",
      "  -2.2363938533370695),\n",
      " ([(0.024365034, 'space'),\n",
      "   (0.013592781, 'edu'),\n",
      "   (0.008306423, 'nasa'),\n",
      "   (0.008152574, 'at'),\n",
      "   (0.006930699, 'widget'),\n",
      "   (0.0065214573, 'earth'),\n",
      "   (0.0062267203, 'mission'),\n",
      "   (0.006164695, 'com'),\n",
      "   (0.0059854174, 'launch'),\n",
      "   (0.005305263, 'orbit'),\n",
      "   (0.0050108116, 'shuttle'),\n",
      "   (0.0050043203, 'satellite'),\n",
      "   (0.0045679715, 'function'),\n",
      "   (0.0044496628, 'on'),\n",
      "   (0.00429181, 'data'),\n",
      "   (0.0042722244, 'gov'),\n",
      "   (0.0040284777, 'surface'),\n",
      "   (0.0039828974, 'c'),\n",
      "   (0.0039083813, 'moon'),\n",
      "   (0.00380169, 'mar')],\n",
      "  -2.2775793262130652),\n",
      " ([(0.03431868, 'please'),\n",
      "   (0.032594755, 'mail'),\n",
      "   (0.02726952, 'for'),\n",
      "   (0.02327503, 'thanks'),\n",
      "   (0.023243241, 'sale'),\n",
      "   (0.021965638, 'me'),\n",
      "   (0.020812511, 'e_mail'),\n",
      "   (0.018379364, 'price'),\n",
      "   (0.017607734, 'offer'),\n",
      "   (0.016718054, 'e'),\n",
      "   (0.015861183, 'am'),\n",
      "   (0.014953358, 'new'),\n",
      "   (0.012369867, 'reply'),\n",
      "   (0.012263456, 'shipping'),\n",
      "   (0.011216372, 'condition'),\n",
      "   (0.010553297, 'hi'),\n",
      "   (0.010019573, 'xterm'),\n",
      "   (0.00978544, 'tape'),\n",
      "   (0.00961056, 'advance'),\n",
      "   (0.009320547, 'send')],\n",
      "  -2.4803685782422),\n",
      " ([(0.32473087, 'x'),\n",
      "   (0.03532459, 'entry'),\n",
      "   (0.028431928, 'n'),\n",
      "   (0.013296987, 'client'),\n",
      "   (0.012443555, 'file'),\n",
      "   (0.011259231, 'c'),\n",
      "   (0.010498522, 'program'),\n",
      "   (0.009730582, 'if'),\n",
      "   (0.009661943, 'y'),\n",
      "   (0.009508129, '0'),\n",
      "   (0.0071627186, 'build'),\n",
      "   (0.006414168, 'winner'),\n",
      "   (0.005830577, 'line'),\n",
      "   (0.0054165553, 'info'),\n",
      "   (0.0051570097, 'we'),\n",
      "   (0.0047383257, 's1'),\n",
      "   (0.004509177, 'dn'),\n",
      "   (0.004487742, 'hare'),\n",
      "   (0.004162698, 'define'),\n",
      "   (0.00410618, 'sweden')],\n",
      "  -5.1651077392692075),\n",
      " ([(0.004477649, 'ethernet_card'),\n",
      "   (0.004144564, 'toshiba'),\n",
      "   (0.0019984613, 'expansion_slot'),\n",
      "   (0.0015446903, 'internal_modem'),\n",
      "   (0.0010973806, 'connectivity'),\n",
      "   (0.0009899305, 'desc'),\n",
      "   (0.00052360265, 'expansion'),\n",
      "   (0.00032033937, 'mnp'),\n",
      "   (0.00013576767, 'ethernet'),\n",
      "   (9.9650715e-06, 'easytalk'),\n",
      "   (9.965035e-06, '3270'),\n",
      "   (9.964999e-06, 't324m'),\n",
      "   (9.964999e-06, 't2rn'),\n",
      "   (9.964999e-06, 't2ll'),\n",
      "   (9.964999e-06, 't1600'),\n",
      "   (9.964999e-06, 't232'),\n",
      "   (9.964999e-06, 't1200'),\n",
      "   (9.9649915e-06, 'transfer_rate'),\n",
      "   (9.96499e-06, '32mb'),\n",
      "   (9.964988e-06, '3b1')],\n",
      "  -6.532659715087317),\n",
      " ([(0.018939035, 'char'),\n",
      "   (0.0089830225, 'coast'),\n",
      "   (0.007388494, 'apr_1993'),\n",
      "   (0.005995947, 'mark'),\n",
      "   (0.0054821116, 'paperback'),\n",
      "   (0.0051342375, 'gmt'),\n",
      "   (0.0050496235, 'panasonic'),\n",
      "   (0.004941105, 'brady'),\n",
      "   (0.0048415414, 'lawrence'),\n",
      "   (0.0045885076, 'wilson'),\n",
      "   (0.004110127, 'ocs'),\n",
      "   (0.0040872027, 'truetype'),\n",
      "   (0.004055783, '922'),\n",
      "   (0.0038592112, 'mark_wilson'),\n",
      "   (0.003795722, 'ocsmd'),\n",
      "   (0.003795722, 'ocs_com'),\n",
      "   (0.003795722, 'mark_ocsmd'),\n",
      "   (0.003540066, 'article_crossposted'),\n",
      "   (0.0035143504, 'crossposted'),\n",
      "   (0.0031993752, 'email_address')],\n",
      "  -9.425519674403164),\n",
      " ([(0.023681723, 'bank'),\n",
      "   (0.0155399935, 'pitt'),\n",
      "   (0.014496143, 'patient'),\n",
      "   (0.014451208, 'gordon'),\n",
      "   (0.011779914, 'gordon_bank'),\n",
      "   (0.01134, 'geb'),\n",
      "   (0.010805804, 'edu'),\n",
      "   (0.009248265, 'headache'),\n",
      "   (0.00716992, 'spin'),\n",
      "   (0.0067615802, 'tension'),\n",
      "   (0.005974524, 'migraine'),\n",
      "   (0.0057685142, 'voluntary'),\n",
      "   (0.0054897782, 'acid'),\n",
      "   (0.0051692873, 'glutamate'),\n",
      "   (0.005004345, 'brain'),\n",
      "   (0.0038089196, 'neurologist'),\n",
      "   (0.003697925, 'rack'),\n",
      "   (0.0036490865, 'cant'),\n",
      "   (0.003555579, 'uucp_uunet'),\n",
      "   (0.0033606899, 'nail')],\n",
      "  -12.631587577804995),\n",
      " ([(0.01794618, 'france'),\n",
      "   (0.012035451, 'ryan'),\n",
      "   (0.006971334, 'bathroom'),\n",
      "   (0.006131133, 'bedroom'),\n",
      "   (0.0047242222, 'nolan'),\n",
      "   (0.0026057751, 'nolan_ryan'),\n",
      "   (0.0019941393, '608'),\n",
      "   (0.0006200068, 'cae'),\n",
      "   (0.00047960546, 'laundry'),\n",
      "   (0.00046512432, 'sublet'),\n",
      "   (0.00033352885, 'furnished'),\n",
      "   (0.00030909473, 'spacious'),\n",
      "   (9.566803e-06, 'norway'),\n",
      "   (9.5668e-06, 'czech'),\n",
      "   (9.566799e-06, 'czech_republic'),\n",
      "   (9.566785e-06, 'koa'),\n",
      "   (9.566785e-06, 'gooden'),\n",
      "   (9.566785e-06, 'nied'),\n",
      "   (9.566785e-06, 'arthroscopic'),\n",
      "   (9.56677e-06, 'yount')],\n",
      "  -13.719485615522215),\n",
      " ([(0.009236018, 'compuserve'),\n",
      "   (0.004400395, 'compuserve_com'),\n",
      "   (0.0031404598, '832'),\n",
      "   (0.0024746885, 'ponder'),\n",
      "   (0.0009822645, 'western_digital'),\n",
      "   (0.00019513136, '4778'),\n",
      "   (9.9124945e-06, 'enviroleague'),\n",
      "   (9.912155e-06, 'critz'),\n",
      "   (9.912134e-06, 'rom_bios'),\n",
      "   (9.912133e-06, 'controller_card'),\n",
      "   (9.912124e-06, 'aol_com'),\n",
      "   (9.912124e-06, 'aol'),\n",
      "   (9.912116e-06, 'peoria'),\n",
      "   (9.912116e-06, '71611'),\n",
      "   (9.912116e-06, '365'),\n",
      "   (9.912113e-06, '864'),\n",
      "   (9.912109e-06, '75300'),\n",
      "   (9.912109e-06, '3115'),\n",
      "   (9.912108e-06, 'st412'),\n",
      "   (9.912108e-06, 'st506')],\n",
      "  -14.312756821246674),\n",
      " ([(0.02219739, 'cd'),\n",
      "   (0.017622786, 'adam'),\n",
      "   (0.012613725, 'ctrl'),\n",
      "   (0.0091664335, 'da'),\n",
      "   (0.0078056087, 'lens'),\n",
      "   (0.0063803154, 'obo'),\n",
      "   (0.0047912286, 'mpc'),\n",
      "   (0.0037526882, 'harvard'),\n",
      "   (0.0036279948, 'pov'),\n",
      "   (0.0030640203, 'compiling'),\n",
      "   (0.003036019, 'cd_rom'),\n",
      "   (0.002944686, 'arafat'),\n",
      "   (0.0029194637, 'cleansing'),\n",
      "   (0.002613036, 'volcano'),\n",
      "   (0.0025574467, 'multimedia'),\n",
      "   (0.0025126413, 'shostack'),\n",
      "   (0.0023958967, 'netland'),\n",
      "   (0.0023021966, 'burton'),\n",
      "   (0.0020202927, 'adam_adam'),\n",
      "   (0.0020202927, 'shostack_adam')],\n",
      "  -14.826469295731519),\n",
      " ([(0.025941176, 'rider'),\n",
      "   (0.015371587, 'suck'),\n",
      "   (0.0111614615, 'beach'),\n",
      "   (0.0059548705, 'lciii'),\n",
      "   (0.0034940084, 'airplane'),\n",
      "   (0.0032097467, 'ported'),\n",
      "   (0.0023730285, 'racer'),\n",
      "   (0.0022854651, 'turtle'),\n",
      "   (0.0015794816, 'std_disclaimer'),\n",
      "   (0.0010864894, 'technician'),\n",
      "   (0.0009814899, 'technician_dr'),\n",
      "   (0.0009814899, 'dod_8177'),\n",
      "   (0.0009814899, '8177'),\n",
      "   (0.00070393167, 'dr'),\n",
      "   (0.00053488236, 'orleans'),\n",
      "   (0.00018922098, '56k'),\n",
      "   (9.347215e-06, 'cub_suck'),\n",
      "   (9.342747e-06, 'sleeve'),\n",
      "   (9.342501e-06, 'promo'),\n",
      "   (9.342486e-06, 'sleeve_10')],\n",
      "  -15.777318564469068)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
