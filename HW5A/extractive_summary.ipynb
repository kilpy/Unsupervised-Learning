{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBLEM 2: Extractive Summarization\n",
    "\n",
    "## Implement the KL-Sum summarization method for each dataset. \n",
    "Follow the ideas in this paper ; you are allowed to use libraries for text cleaning, segmentation into sentences, etc. In this problem PS stands for \"growing\" summary distribution, while PD stands for fixed document distribution. Run it twice :\n",
    "\n",
    "A) PS and PD are over words, proportional to counts of words\n",
    "\n",
    "B) PD and PS distributions over topics, instead of distributions over words. LDA will give you the PD topic distribution at training; for PS you can call LDA[summary] (without retraining, treat summary as \"new_doc\")\n",
    "\n",
    "For DUC dataset evaluate against human gold summaries with ROUGE. ROUGE Perl package. Use the \"Abstract\" part of the files ins folder \"Summaries\" as the gold summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffb1b6d153040f989eb75f869b0d000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/714k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544298c7db064835ad41062f02fbc69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"midas/duc2001\", \"raw\")\n",
    "\n",
    "# print the first example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "308\n",
      "dict_keys(['test'])\n",
      "{'id': 'AP881222-0089', 'document': ['Here', ',', 'at', 'a', 'glance', ',', 'are', 'developments', 'today', 'involving', 'the', 'crash', 'of', 'Pan', 'American', 'World', 'Airways', 'Flight', '103', 'Wednesday', 'night', 'in', 'Lockerbie', ',', 'Scotland', ',', 'that', 'killed', 'all', '259', 'people', 'aboard', 'and', 'more', 'than', '20', 'people', 'on', 'the', 'ground', ':'], 'doc_bio_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'extractive_keyphrases': ['pan american world airways flight 103', 'crash', 'lockerbie'], 'abstractive_keyphrases': ['terrorist threats', 'widespread wreckage', 'radical palestinian faction', 'terrorist bombing', 'bomb threat', 'sabotage'], 'other_metadata': {'text': [], 'bio_tags': []}}\n"
     ]
    }
   ],
   "source": [
    "print(type(ds['test'][0]))\n",
    "print(len(ds['test']))\n",
    "print(ds.keys())\n",
    "print(ds['test'][0]) # print the first example in the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20ng dataset\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "ng = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
