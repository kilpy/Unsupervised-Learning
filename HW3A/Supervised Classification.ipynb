{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Datasets:\n",
    "- MNIST\n",
    "- Spambase\n",
    "- 20NG\n",
    "\n",
    "For each, perform 2 classification algorithms:\n",
    "- L2-reg Logistic Regression\n",
    "- Decision Trees\n",
    "\n",
    "\n",
    "allowed to use library for classification. \n",
    "\n",
    "allowed to use library to process data in appropriate formats.\n",
    "\n",
    "You are required to explain/analyze the model trained in terms of features : for each of the 6 runs list the top F=30 features. \n",
    "\n",
    "For the Regression these correspond to the highest-absolute-value F coefficients; \n",
    "\n",
    "for Decision Tree they are the first F splits. In particular for Decision Tree on 20NG, report performance for two tree sizes ( by depths of the tree, or number of leaves, or number of splits )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as torch\n",
    "from sklearn.datasets import _twenty_newsgroups\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='data', download=True, transform=transforms.ToTensor())\n",
    "features = mnist_dataset.data\n",
    "labels = mnist_dataset.targets\n",
    "\n",
    "# Normalize the features\n",
    "features = features / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 2.0248\n",
      "Epoch [2/5], Loss: 1.9195\n",
      "Epoch [3/5], Loss: 1.8643\n",
      "Epoch [4/5], Loss: 1.8290\n",
      "Epoch [5/5], Loss: 1.8038\n",
      "Accuracy of the model on the MNIST test images: 84.15%\n"
     ]
    }
   ],
   "source": [
    "# using pytorch to perform logistic regression:\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "    \n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = 28 * 28  # MNIST images are 28x28 pixels\n",
    "output_dim = 10  # 10 classes for digits 0-9\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare the data for training\n",
    "features = features.view(-1, input_dim).float()\n",
    "labels = labels.long()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "num_batches = len(features) // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(num_batches):\n",
    "        # Get the batch data\n",
    "        batch_features = features[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(num_batches):\n",
    "        batch_features = features[i * batch_size:(i + 1) * batch_size]\n",
    "        batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "        outputs = model(batch_features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "print(f'Accuracy of the model on the MNIST test images: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 features for each class:\n",
      "Class 0: [357 404 432 377 491 460 405 352 489 488 437 490 351 381 353 379 436 463\n",
      " 461 433 462 464 435 378 406 409 380 407 434 408]\n",
      "Class 1: [516 347 517 294 521 430 374 433 466 437 403 377 346 354 381 402 409 489\n",
      " 351 379 490 462 434 322 323 461 295 350 378 406]\n",
      "Class 2: [343 265 322 569 376 581 568 540 321 659 153 349 375 316 373 350 318 320\n",
      " 317 291 348 290 656 345 347 372 657 374 346 319]\n",
      "Class 3: [344 544 491 377 572 486 516 545 485 542 460 291 317 461 262 487 490 514\n",
      " 546 289 349 350 543 515 462 488 518 290 489 517]\n",
      "Class 4: [237 399 372 240 598 543 464 570 402 430 400 463 436 184 373 428 401 542\n",
      " 374 427 183 209 212 569 211 429 238 182 239 210]\n",
      "Class 5: [488 461 190 382 296 376 381 516 487 328 319 320 297 346 489 299 355 189\n",
      " 514 356 375 348 354 219 298 325 327 374 326 347]\n",
      "Class 6: [513 268 572 296 573 272 240 626 213 297 243 244 571 655 514 486 215 299\n",
      " 271 656 515 542 241 214 657 298 543 242 270 269]\n",
      "Class 7: [403 379 268 431 181 186 269 458 153 267 260 154 429 266 298 183 374 461\n",
      " 155 184 182 433 430 402 377 378 404 432 406 405]\n",
      "Class 8: [514 546 601 460 455 575 466 437 488 405 515 429 461 296 411 410 400 404\n",
      " 377 487 545 656 401 268 574 376 428 438 573 267]\n",
      "Class 9: [573 294 372 464 600 599 624 569 354 409 436 210 571 544 158 381 568 353\n",
      " 211 155 212 596 626 597 542 154 570 598 157 156]\n"
     ]
    }
   ],
   "source": [
    "# Analyze model in terms of features, d = top 30 features\n",
    "def analyze_model(model, input_dim):\n",
    "    weights = model.linear.weight.data.numpy()\n",
    "    top_features = np.argsort(np.abs(weights), axis=1)[:, -30:]  # Get top 30 features for each class\n",
    "    return top_features\n",
    "top_features = analyze_model(model, input_dim)\n",
    "\n",
    "print(\"Top 30 features for each class:\")\n",
    "for i in range(output_dim):\n",
    "    print(f\"Class {i}: {top_features[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n",
      "Accuracy of the Decision Tree Classifier on the MNIST dataset: 87.76%\n"
     ]
    }
   ],
   "source": [
    "# Using decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_dataset_train = datasets.MNIST(root='data', train = True, download=True)\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root='data', train = False, download=True)\n",
    "print(mnist_dataset_train.data.shape)\n",
    "print(mnist_dataset.data.shape)\n",
    "# Train features, labels\n",
    "features = mnist_dataset_train.data\n",
    "labels = mnist_dataset_train.targets\n",
    "\n",
    "# Test features, labels\n",
    "test_features = mnist_dataset.data\n",
    "test_labels = mnist_dataset.targets\n",
    "\n",
    "# convert to numpy arrays:\n",
    "features = features.numpy()\n",
    "labels = labels.numpy()\n",
    "\n",
    "test_features = test_features.numpy()\n",
    "test_labels = test_labels.numpy()\n",
    "\n",
    "# Normalize the features\n",
    "features = features / 255.0\n",
    "test_features = test_features / 255.0\n",
    "\n",
    "# Reshape the features to 2D array\n",
    "features = features.reshape(features.shape[0], -1)\n",
    "test_features = test_features.reshape(test_features.shape[0], -1)\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "dt_classifier.fit(features, labels)\n",
    "\n",
    "# Predict on the testing set\n",
    "dt_predictions = dt_classifier.predict(test_features)\n",
    "\n",
    "# Calculate accuracy\n",
    "dt_accuracy = accuracy_score(test_labels, dt_predictions)\n",
    "print(f'Accuracy of the Decision Tree Classifier on the MNIST dataset: {dt_accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spambase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2    3     4     5     6     7     8     9   ...     48  \\\n",
       "0     0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "1     0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.000   \n",
       "2     0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.010   \n",
       "3     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "4     0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.000   \n",
       "...    ...   ...   ...  ...   ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "4596  0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4597  0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4598  0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.102   \n",
       "4599  0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "4600  0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  ...  0.000   \n",
       "\n",
       "         49   50     51     52     53     54   55    56  57  \n",
       "0     0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1     0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2     0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3     0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4     0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "...     ...  ...    ...    ...    ...    ...  ...   ...  ..  \n",
       "4596  0.232  0.0  0.000  0.000  0.000  1.142    3    88   0  \n",
       "4597  0.000  0.0  0.353  0.000  0.000  1.555    4    14   0  \n",
       "4598  0.718  0.0  0.000  0.000  0.000  1.404    6   118   0  \n",
       "4599  0.057  0.0  0.000  0.000  0.000  1.147    5    78   0  \n",
       "4600  0.000  0.0  0.125  0.000  0.000  1.250    5    40   0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambase = pd.read_csv('spambase.data', header=None, sep=',')\n",
    "spambase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "s_labels = spambase.iloc[:, -1]\n",
    "s_features = spambase.iloc[:, :-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the features\n",
    "s_features = (s_features - s_features.mean()) / s_features.std()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "s_features_train, s_features_test, s_labels_train, s_labels_test = train_test_split(s_features, s_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model on the Spambase dataset: 91.97%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Using logistic regression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(s_features_train, s_labels_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "logistic_predictions = logistic_model.predict(s_features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "logistic_accuracy = accuracy_score(s_labels_test, logistic_predictions)\n",
    "print(f'Accuracy of the Logistic Regression model on the Spambase dataset: {logistic_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier on the Spambase dataset: 91.31%\n"
     ]
    }
   ],
   "source": [
    "# Decision tree classifier\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "dt_classifier.fit(s_features_train, s_labels_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "dt_predictions = dt_classifier.predict(s_features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "dt_accuracy = accuracy_score(s_labels_test, dt_predictions)\n",
    "print(f'Accuracy of the Decision Tree Classifier on the Spambase dataset: {dt_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20NG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train = _twenty_newsgroups.fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "ng_train_feat = ng_train.data\n",
    "ng_train_labels = ng_train.target\n",
    "\n",
    "ng_test = _twenty_newsgroups.fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "ng_test_feat = ng_test.data\n",
    "ng_test_labels = ng_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TFIDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the training features\n",
    "ng_train_features = tfidf_vectorizer.fit_transform(ng_train_feat)\n",
    "\n",
    "# Transform the testing features\n",
    "ng_test_features = tfidf_vectorizer.transform(ng_test_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model on the 20 Newsgroups dataset: 64.83%\n"
     ]
    }
   ],
   "source": [
    "# Using logistic regression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "logistic_model.fit(ng_train_features, ng_train_labels)\n",
    "\n",
    "# Predict on the testing set\n",
    "logistic_predictions = logistic_model.predict(ng_test_features)\n",
    "\n",
    "# Calculate accuracy\n",
    "logistic_accuracy = accuracy_score(ng_test_labels, logistic_predictions)\n",
    "print(f'Accuracy of the Logistic Regression model on the 20 Newsgroups dataset: {logistic_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Decision Tree Classifier on the 20 Newsgroups dataset: 43.02%\n",
      "Top 30 features for each class:\n",
      "Class 0: [ 813 4770 2443 4009  975 3639 2563  701 1384 4489 2645 2971 4232  535\n",
      " 2528 3958 3771 3130 4481 2972 3597  589  724 2440  590 2439  766 3769\n",
      " 2047  588]\n",
      "Class 1: [4971 2947 2185 2716 4407 1068 2652 1958 4194 3249 1061 4768 3539 1130\n",
      " 4246 4766 1857 4777 3418  423 3424 1313  466 4551 1925 1859 2296  176\n",
      " 2295 2075]\n",
      "Class 2: [1908 1458  961 2769 2868 4762 3005 4708  336 2992  951  901 2360 1544\n",
      " 2897 1958 4716 3114 3098 1909 1564 3875 3517 1859 4890 1563 1003  635\n",
      " 1857 4893]\n",
      "Class 3: [  58 2868 1565 1544 1261 2177 1056 2964  902 2154  763 2264 3220 1500\n",
      " 2434 2436 2980 4799 1988  736 3431  842  198 1206 1561 2270 2963  901\n",
      " 3308 4005]\n",
      "Class 4: [2863 2142 4065 2760 3311 2950  862 1939 4174    6 2406 2762 1047 4143\n",
      " 2288 4005 4812 2963 3517 3115 1578 4144 2602 3453 1561  939 3621 4008\n",
      "  499 2758]\n",
      "Class 5: [3541 4708 3415 4500 1706 4214 1070 2646 3378 3927 4957 4716 4076 4713\n",
      " 3184 1061 2185 1502 1042 4966 4883  500 4390 2937 4959 4970 4882 2981\n",
      " 4070 4892]\n",
      "Class 6: [4529 4598 1906 2794 4709 2769 4389  930  129 1544 1561 1720 1983 1930\n",
      " 3132 2716 3156 4039 2328 3491 1633  561 2399 3073    0 1143 4102 4038\n",
      " 3154 3947]\n",
      "Class 7: [4754 3491  337 4753 2625 1426 2903  307 2643 2949 1566 3701  416 3963\n",
      " 4105 4561 2227 4876 3886  623  619 4816 2099 4592 1353 1653 3166 1918\n",
      "  913  900]\n",
      "Class 8: [2588 2705 2155 1819 4559 4013  774 3886 2184 1566 1560  946 4448 3360\n",
      " 3701 4576 3866 1225 3865 2227 2986 1532  760 3868 2176 2985 3864  730\n",
      " 1528  729]\n",
      "Class 9: [3372 4977 3088 1515  904 1038 3371 4012  677 1804 1679 2614 4219 2886\n",
      " 3373 1983 2199 1982 3346 2474  434  660 3402  796 3928 1298 4266 4471\n",
      " 4981  671]\n",
      "Class 10: [2046 2153 4895 1898 3317 2041 2614 1454 3375 1689 2266 1444 1302 1059\n",
      " 4472 3897 3404 3666 1983 3405 2613 3402 3593 2818 3399 4012 3081 1982\n",
      " 4471 2211]\n",
      "Class 11: [3663 3587 1613 1034 1406 1287  375 4026 1291  443  423 3348 1686 4020\n",
      " 3918 4803 1643 3340 1418 1045 1289 3509  984 4027 2533 2061 3113 2531\n",
      " 1644 1046]\n",
      "Class 12: [3363  829  984 2675 1103 1440 4639 4708 1222 2732 3654 4729 1010  610\n",
      "  918 3991 4709 3232 4902 1305  451 2987 3655  985 4651 2090 4806 3452\n",
      " 1628 1009]\n",
      "Class 13: [3299  667  924 1615 4441 1998 2355 4868 2056 4164 3988 1911 4428 1773\n",
      "  752 3986 1617 2859 4429  884 1910 1465 2159 3300 4616 3255 2858 1498\n",
      " 3007 1520]\n",
      "Class 14: [3136 2561 1240 4165 3985  389 2575 3204 3293 2508 2814 3545 3893  428\n",
      " 1349 3894 1968 3511 3959 1890 4195 2740 4121 1587 4222 2970 2592 3036\n",
      " 3203 4221]\n",
      "Class 15: [2578 3901 1834 4248 3939 3769  703 3628  771 2730  616 2173 2721 2166\n",
      " 3842  922 4148 4633 4003 1591 1794 2812  724  995  994  993 2470  996\n",
      "  999 2047]\n",
      "Class 16: [4037 4862 3318 4449 2476 1830 4514 1123 1987 1075 1344 3871 1280 2542\n",
      "  587 3419 2906  687 4827 2595 1874 2061  678 3109 4857 1875 1823 4858\n",
      " 2109 2108]\n",
      "Class 17: [3944 3260 3318 1021 2084 1993 1948  530 4197 2217  374  542  360 2446\n",
      " 3312 3259 2622 4645 3004 2474 4064 4643  518  543  544  517 4644 2475\n",
      " 2445 2444]\n",
      "Class 18: [1277  361 2596 2540 3695 1915 1710 3282 4084 4980  746 4460 2159 2648\n",
      " 4289 1156 1991 2224 2478  843 2869 4285 4618 2226 3480 3318 1571 1045\n",
      " 4459 2061]\n",
      "Class 19: [1945 2605 2721 2476 2255  975  324 4979  702  993 3769 1710 2249 4533\n",
      "  982 1189 4741 2971 4660 1823 2972  724 3206  996 2528 3130 2563  994\n",
      " 2470 2047]\n",
      "Top 30 features for each class:\n",
      "Class 0: ['broken', 'vice', 'isn', 'sea', 'cheers', 'qur', 'koresh', 'belief', 'define', 'tells', 'liar', 'moral', 'species', 'argument', 'kent', 'satan', 'religious', 'objective', 'tek', 'morality', 'punishment', 'atheist', 'bible', 'islamic', 'atheists', 'islam', 'bobby', 'religion', 'god', 'atheism']\n",
      "Class 1: ['xv', 'mode', 'hi', 'looking', 'surface', 'color', 'library', 'ftp', 'software', 'package', 'code', 'vga', 'program', 'computer', 'sphere', 'vesa', 'file', 'viewer', 'points', 'algorithm', 'polygon', 'cview', 'animation', 'tiff', 'format', 'files', 'images', '3d', 'image', 'graphics']\n",
      "Class 2: ['font', 'diamond', 'characters', 'mail', 'memory', 'version', 'ms', 'use', 'access', 'mouse', 'change', 'card', 'ini', 'dos', 'microsoft', 'ftp', 'using', 'nt', 'norton', 'fonts', 'drivers', 'risc', 'problem', 'files', 'win', 'driver', 'cica', 'ax', 'file', 'windows']\n",
      "Class 3: ['17', 'memory', 'drives', 'dos', 'cpu', 'help', 'cmos', 'monitors', 'cards', 'hd', 'board', 'ibm', 'os', 'disk', 'irq', 'isa', 'motherboard', 'vlb', 'gateway', 'bios', 'port', 'bus', '486', 'controller', 'drive', 'ide', 'monitor', 'card', 'pc', 'scsi']\n",
      "Class 4: ['meg', 'hardware', 'serial', 'machines', 'pds', 'modem', 'cable', 'fpu', 'slot', '040', 'internal', 'macs', 'clock', 'simm', 'iisi', 'scsi', 'vram', 'monitor', 'problem', 'nubus', 'duo', 'simms', 'lc', 'powerbook', 'drive', 'centris', 'quadra', 'se', 'apple', 'mac']\n",
      "Class 5: ['programming', 'use', 'pointer', 'terminals', 'event', 'source', 'colormap', 'lib', 'pixmap', 'running', 'x11', 'using', 'set', 'user', 'openwindows', 'code', 'hi', 'display', 'clients', 'xlib', 'widgets', 'application', 'sun', 'mit', 'x11r5', 'xterm', 'widget', 'motif', 'server', 'window']\n",
      "Class 6: ['think', 'trade', 'following', 'manuals', 'used', 'mail', 'summer', 'cd', '25', 'dos', 'drive', 'excellent', 'games', 'forsale', 'obo', 'looking', 'offers', 'selling', 'includes', 'price', 'email', 'asking', 'interested', 'new', '00', 'condition', 'shipping', 'sell', 'offer', 'sale']\n",
      "Class 7: ['vehicles', 'price', 'accident', 'vehicle', 'left', 'designed', 'mileage', '__', 'lh', 'models', 'driving', 'rear', 'air', 'saturn', 'sho', 'tires', 'honda', 'wheel', 'road', 'autos', 'auto', 'vw', 'gt', 'toyota', 'dealer', 'engine', 'oil', 'ford', 'cars', 'car']\n",
      "Class 8: ['later', 'lock', 'head', 'fault', 'tire', 'seat', 'boots', 'road', 'hey', 'driving', 'drink', 'chain', 'tank', 'piece', 'rear', 'tony', 'riders', 'cop', 'rider', 'honda', 'motorcycles', 'dog', 'bmw', 'riding', 'helmet', 'motorcycle', 'ride', 'bikes', 'dod', 'bike']\n",
      "Class 9: ['pitchers', 'yankees', 'nl', 'dl', 'career', 'clemens', 'pitcher', 'season', 'bat', 'fan', 'era', 'league', 'sox', 'mets', 'pitching', 'games', 'hit', 'game', 'phillies', 'jewish', 'alomar', 'ball', 'players', 'braves', 'runs', 'cubs', 'stadium', 'team', 'year', 'baseball']\n",
      "Class 10: ['goals', 'hawks', 'wings', 'flyers', 'pens', 'gm', 'league', 'devils', 'pittsburgh', 'espn', 'ice', 'detroit', 'cup', 'coach', 'teams', 'roger', 'playoff', 'rangers', 'games', 'playoffs', 'leafs', 'players', 'puck', 'mask', 'play', 'season', 'nhl', 'game', 'team', 'hockey']\n",
      "Class 11: ['random', 'public', 'eff', 'classified', 'denning', 'crypt', 'administration', 'secure', 'cryptography', 'amendment', 'algorithm', 'phone', 'escrow', 'secret', 'rsa', 'voice', 'encrypted', 'pgp', 'des', 'clinton', 'crypto', 'privacy', 'chip', 'security', 'keys', 'government', 'nsa', 'key', 'encryption', 'clipper']\n",
      "Class 12: ['pin', 'build', 'chip', 'lines', 'company', 'detector', 'tube', 'use', 'cooling', 'low', 'radar', 'uv', 'circuits', 'audio', 'catalog', 'scope', 'used', 'output', 'wire', 'current', 'amp', 'motorola', 'radio', 'chips', 'tv', 'ground', 'voltage', 'power', 'electronics', 'circuit']\n",
      "Class 13: ['patient', 'banks', 'cause', 'effective', 'taking', 'geb', 'information', 'weight', 'gordon', 'skin', 'scientific', 'foods', 'symptoms', 'eye', 'blood', 'science', 'effects', 'medicine', 'syndrome', 'cancer', 'food', 'diet', 'health', 'patients', 'treatment', 'pain', 'medical', 'disease', 'msg', 'doctor']\n",
      "Class 14: ['observatory', 'known', 'cost', 'sky', 'sci', 'aerospace', 'landing', 'orbital', 'pat', 'jupiter', 'mars', 'project', 'rocket', 'allen', 'dc', 'rockets', 'funding', 'prize', 'satellite', 'flight', 'solar', 'lunar', 'shuttle', 'earth', 'spacecraft', 'moon', 'launch', 'nasa', 'orbit', 'space']\n",
      "Class 15: ['language', 'roman', 'feel', 'spiritual', 'sabbath', 'religion', 'believe', 'question', 'book', 'love', 'authority', 'hell', 'lord', 'heaven', 'resurrection', 'catholic', 'sin', 'truth', 'scripture', 'easter', 'faith', 'marriage', 'bible', 'christianity', 'christian', 'christ', 'jesus', 'christians', 'church', 'god']\n",
      "Class 16: ['self', 'weaver', 'people', 'tanks', 'jim', 'federal', 'texas', 'compound', 'gas', 'com', 'davidians', 'rights', 'criminals', 'killed', 'atf', 'police', 'militia', 'bd', 'waco', 'law', 'firearm', 'government', 'batf', 'nra', 'weapon', 'firearms', 'fbi', 'weapons', 'guns', 'gun']\n",
      "Class 17: ['said', 'palestinians', 'people', 'civilians', 'greeks', 'gaza', 'freedom', 'argic', 'soldiers', 'holocaust', 'adl', 'armenia', 'adam', 'israelis', 'peace', 'palestinian', 'lebanon', 'turks', 'mr', 'jewish', 'serdar', 'turkey', 'arabs', 'armenian', 'armenians', 'arab', 'turkish', 'jews', 'israeli', 'israel']\n",
      "Class 18: ['crime', 'adams', 'laws', 'kids', 'reagan', 'force', 'evidence', 'party', 'sexual', 'yeah', 'blacks', 'taxes', 'health', 'libertarian', 'states', 'congress', 'gay', 'homosexual', 'jobs', 'bush', 'men', 'state', 'trial', 'homosexuals', 'president', 'people', 'drugs', 'clinton', 'tax', 'government']\n",
      "Class 19: ['frank', 'lds', 'lord', 'jim', 'humans', 'cheers', 'abortion', 'ye', 'beliefs', 'christ', 'religion', 'evidence', 'hudson', 'thou', 'children', 'context', 'values', 'moral', 'tyre', 'fbi', 'morality', 'bible', 'order', 'christians', 'kent', 'objective', 'koresh', 'christian', 'jesus', 'god']\n"
     ]
    }
   ],
   "source": [
    "# Decision tree classifier\n",
    "\n",
    "# Initialize the Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Tree Classifier\n",
    "dt_classifier.fit(ng_train_features, ng_train_labels)\n",
    "\n",
    "# Predict on the testing set\n",
    "dt_predictions = dt_classifier.predict(ng_test_features)\n",
    "\n",
    "# Calculate accuracy\n",
    "dt_accuracy = accuracy_score(ng_test_labels, dt_predictions)\n",
    "print(f'Accuracy of the Decision Tree Classifier on the 20 Newsgroups dataset: {dt_accuracy * 100:.2f}%')\n",
    "\n",
    "# get the top 30 features\n",
    "def analyze_model(model, input_dim):\n",
    "    weights = model.coef_\n",
    "    top_features = np.argsort(np.abs(weights), axis=1)[:, -30:]  # Get top 30 features for each class\n",
    "    return top_features\n",
    "top_features = analyze_model(logistic_model, ng_train_features.shape[1])\n",
    "print(\"Top 30 features for each class:\")\n",
    "\n",
    "for i in range(len(ng_train.target_names)):\n",
    "    print(f\"Class {i}: {top_features[i]}\")\n",
    "\n",
    "# unpack features thru the tfidf vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "top_features = np.array(top_features)\n",
    "top_features_names = []\n",
    "for i in range(len(ng_train.target_names)):\n",
    "    top_features_names.append([feature_names[j] for j in top_features[i]])\n",
    "print(\"Top 30 features for each class:\")\n",
    "\n",
    "for i in range(len(ng_train.target_names)):\n",
    "    print(f\"Class {i}: {top_features_names[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: PCA Using library (sklearn compatible)\n",
    "\n",
    "A) For MNIST dataset, run a PCA-library to get data on D=5 features. Rerun the classification tasks from PB1, compare testing performance with the one from PB1. Then repeat this exercise for D=20\n",
    "\n",
    "B) Run PCA library on Spambase and repeat one of the classification algorithms. What is the smallest D (number of PCA dimensions) you need to get a comparable test result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torchvision import datasets, transforms\n",
    "from pca import pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root='data', download=True, transform=transforms.ToTensor())\n",
    "features = mnist_dataset.data\n",
    "labels = mnist_dataset.targets\n",
    "\n",
    "# Normalize the features\n",
    "features = features / 255.0\n",
    "\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.09704393 0.07096103 0.06169181 0.05389257 0.04867888]\n"
     ]
    }
   ],
   "source": [
    "# a) use PCA to reduce the dimensionality of the data to d = 5:\n",
    "pca = PCA(n_components=5)\n",
    "features_pca = pca.fit_transform(features.view(-1, 28 * 28).numpy())\n",
    "\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 5)\n"
     ]
    }
   ],
   "source": [
    "# data now looks like:\n",
    "print(features_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model on the MNIST dataset after PCA d = 5: 66.95%\n"
     ]
    }
   ],
   "source": [
    "# Perform training, testing\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features_pca, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "predictions = model.predict(features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print(f'Accuracy of the Logistic Regression model on the MNIST dataset after PCA d = 5: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model on the MNIST dataset after PCA d = 20: 87.51%\n"
     ]
    }
   ],
   "source": [
    "# d = 20\n",
    "pca = PCA(n_components=20)\n",
    "features_pca = pca.fit_transform(features.view(-1, 28 * 28).numpy())\n",
    "\n",
    "# Perform training, testing\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features_pca, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "predictions = model.predict(features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print(f'Accuracy of the Logistic Regression model on the MNIST dataset after PCA d = 20: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Logistic Regression model on the Spambase dataset after PCA d = 1: 84.04%\n",
      "Explained variance ratio: [0.11564794]\n"
     ]
    }
   ],
   "source": [
    "# B) Run PCA library on Spambase and repeat one of the classification algorithms. What is the smallest D (number of PCA dimensions) you need to get a comparable test result?\n",
    "\n",
    "# Load the Spambase dataset\n",
    "spambase = pd.read_csv('spambase.data', header=None, sep=',')\n",
    "s_labels = spambase.iloc[:, -1]\n",
    "s_features = spambase.iloc[:, :-1]\n",
    "\n",
    "# Normalize the features\n",
    "s_features = (s_features - s_features.mean()) / s_features.std()\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=1)\n",
    "s_features_pca = pca.fit_transform(s_features)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "s_features_train, s_features_test, s_labels_train, s_labels_test = train_test_split(s_features_pca, s_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(s_features_train, s_labels_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "predictions = model.predict(s_features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(s_labels_test, predictions)\n",
    "print(f'Accuracy of the Logistic Regression model on the Spambase dataset after PCA d = 1: {accuracy * 100:.2f}%')\n",
    "\n",
    "# what is this feature?\n",
    "print(f'Explained variance ratio: {pca.explained_variance_ratio_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Self Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load mnist dataset\n",
    "mnist_dataset = datasets.MNIST(root='data', download=True, transform=transforms.ToTensor())\n",
    "features = mnist_dataset.data\n",
    "labels = mnist_dataset.targets\n",
    "\n",
    "# Normalize the features\n",
    "features = features / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.09704664+0.j 0.07095924+0.j 0.06169089+0.j 0.05389419+0.j\n",
      " 0.04868797+0.j]\n"
     ]
    }
   ],
   "source": [
    "# get covariance matrix\n",
    "cov_matrix = np.cov(features.view(-1, 28 * 28).numpy().T)\n",
    "\n",
    "# get eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "# sort the eigenvalues in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# get explained variance ratio\n",
    "explained_variance_ratio = eigenvalues / eigenvalues.sum()\n",
    "print(f'Explained variance ratio: {explained_variance_ratio[:5]}')\n",
    "\n",
    "# get the top 5 eigenvectors\n",
    "top_eigenvectors = eigenvectors[:, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Complex data not supported\n[[-3.87895452+0.j -0.62961559+0.j -1.24062249+0.j -4.07137788+0.j\n  -2.75597119+0.j]\n [-4.9482789 +0.j -0.10051405+0.j  1.82497409+0.j -2.08015664+0.j\n   0.3038148 +0.j]\n [ 0.62456308+0.j -2.89567003+0.j  2.16542742+0.j -1.69270534+0.j\n  -3.43336784+0.j]\n ...\n [-7.74606021+0.j -1.70969164+0.j  3.93920483+0.j -2.92376408+0.j\n  -1.65427458+0.j]\n [-6.05845876+0.j -4.26704566+0.j  4.0789865 +0.j -2.39895482+0.j\n  -2.85616671+0.j]\n [-2.21837206+0.j  1.39129954+0.j  1.7959887 +0.j -0.60472536+0.j\n  -4.35491592+0.j]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mComplexWarning\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1055\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1055\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_array_api.py:839\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 839\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n",
      "\u001b[1;31mComplexWarning\u001b[0m: Casting complex values to real discards the imaginary part",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Predict on the testing set\u001b[39;00m\n\u001b[0;32m     13\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1222\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1230\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1231\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2959\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2960\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2961\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1370\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1387\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1389\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\validation.py:1057\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1055\u001b[0m             array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[1;32m-> 1057\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1058\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1059\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# It is possible that the np.array(..) gave no warning. This happens\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# when no dtype conversion happened, for example dtype = None. The\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# result is that np.array(..) produces an array of complex dtype\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# and we need to catch and raise exception for such cases.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m _ensure_no_complex_data(array)\n",
      "\u001b[1;31mValueError\u001b[0m: Complex data not supported\n[[-3.87895452+0.j -0.62961559+0.j -1.24062249+0.j -4.07137788+0.j\n  -2.75597119+0.j]\n [-4.9482789 +0.j -0.10051405+0.j  1.82497409+0.j -2.08015664+0.j\n   0.3038148 +0.j]\n [ 0.62456308+0.j -2.89567003+0.j  2.16542742+0.j -1.69270534+0.j\n  -3.43336784+0.j]\n ...\n [-7.74606021+0.j -1.70969164+0.j  3.93920483+0.j -2.92376408+0.j\n  -1.65427458+0.j]\n [-6.05845876+0.j -4.26704566+0.j  4.0789865 +0.j -2.39895482+0.j\n  -2.85616671+0.j]\n [-2.21837206+0.j  1.39129954+0.j  1.7959887 +0.j -0.60472536+0.j\n  -4.35491592+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# Perform training, testing\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features.view(-1, 28 * 28).numpy().dot(top_eigenvectors), labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(features_train, labels_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "predictions = model.predict(features_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print(f'Accuracy of the Logistic Regression model on the MNIST dataset after PCA d = 5: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
