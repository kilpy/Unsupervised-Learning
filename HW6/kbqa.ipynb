{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e9bd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util functions\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to get the cosine similarity between a relation and query\n",
    "# Note: Update this string with the path to the file\n",
    "word2vec_model = gensim.models.Word2Vec.load('word2vec_train_dev.dat')\n",
    "\n",
    "\n",
    "def get_rel_score_word2vecbase(rel: str, query: str) -> float:\n",
    "    \"\"\"\n",
    "    Get score for query and relation. Used to inform exploration of knowledge graph.\n",
    "\n",
    "    :param rel: relation, or edge in knowledge graph\n",
    "    :param query: query, question to answer\n",
    "    :return: float score similarity between question and relation\n",
    "    \"\"\"\n",
    "    # Relation not in embedding vocabulary\n",
    "    if rel not in word2vec_model.wv:\n",
    "        return 0.0\n",
    "    # Relation must start with ns:\n",
    "    rel = 'ns:' + rel if not rel[:3] == 'ns:' else rel\n",
    "\n",
    "    words = word_tokenize(query.lower())\n",
    "    w_embs = []\n",
    "    for w in words:\n",
    "        if w in word2vec_model.wv:\n",
    "            w_embs.append(word2vec_model.wv[w])\n",
    "    return np.mean(cosine_similarity(w_embs, [word2vec_model.wv[rel]]))\n",
    "\n",
    "\n",
    "def load_node_label_lookup(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    Load the lookup dictionary for nodes from the provided json file.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the json file containing the lookup dictionary.\n",
    "\n",
    "    Returns: Dictionary of node ids to text description of node.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return json.load(fp)\n",
    "\n",
    "\n",
    "def load_query_df(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Load a simplified dataframe of queries. Generated from the original queries nested dictionary, this simplified\n",
    "    version contains all necessary information for performing the graph traversal testing without all the extra\n",
    "    information and difficult formatting. Simply loop through this dataframe row by row, start at the start node with\n",
    "    the query for that row, and the expected answers are given in that same row.\n",
    "\n",
    "    Args:\n",
    "        filepath: Path to the provided parquet file\n",
    "\n",
    "    Returns: Dataframe of queries to perform on the graph.\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.read_parquet(filepath)\n",
    "\n",
    "\n",
    "# Function to load the graph from file\n",
    "def load_graph() -> dict:\n",
    "    \"\"\"\n",
    "\n",
    "    Load the graph from the given file.\n",
    "\n",
    "    Returns: Graph, in form of node_id key, and nested list value. Nested list is adjacency list, with each list\n",
    "    containing the relation, and destination node_id.\n",
    "\n",
    "    \"\"\"\n",
    "    # Preparing the graph\n",
    "    graph = defaultdict(list)\n",
    "    for line in open('graph.txt'):\n",
    "        line = eval(line[:-1])\n",
    "        graph[line[0]].append([line[1], line[2]])\n",
    "    return graph\n",
    "\n",
    "\n",
    "# Function to load the queries from file\n",
    "# Preparing the queries\n",
    "def load_queries() -> list:\n",
    "    \"\"\"\n",
    "\n",
    "    Load the original queries file. This format can be extremely confusing, for a simplified format use load_query_df.\n",
    "\n",
    "    Returns: Nested list, with index, node_id, relation types for answers, text description of start node, and dict of\n",
    "    answers.\n",
    "\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    for line in open('annotations.txt'):\n",
    "        line = eval(line[:-1])\n",
    "        queries.append(line)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given is knowledge graph with entities and relations, questions with starting entity and answers, and their word embedding.\n",
    "# Also we provide a json file for lookup of vertex IDs.\n",
    "# For each question, navigate the graph from the start entity outwards until you find appropriate answer entities.\n",
    "# Utils functions (similarity, load_graphs) are given, but you can choose not to use them.\n",
    "# This python file contains the helper functions for this homework, the only update needed to use this file is to fill in the file paths.\n",
    "\n",
    "# - The number of correct answers varies (could be 1, could many), use F1 to compare your answers with the given correct answers\n",
    "# - Utils functions (similarity, load_graphs) are given, but you can choose not to use them.\n",
    "# - Answers are given to be used for evaluation only, DO NOT USE ANSWERS IN YOUR GRAPH TRAVERSAL. \n",
    "# Your strategy should be a graph traversal augmented with scoring of paths; you might discard paths not promising along the way.\n",
    "# This is similar to a focused crawl strategy. You will take a query (question) that you are trying to answer, it will have a starting entity. \n",
    "# Begin your traversal at that starting entity, and look at all adjacent edges. \n",
    "# Use get_rel_score_word2vec base to get a similarity score for each edge, and traverse the edges that are promising. \n",
    "# This part is up to you, you can cut off scores below a certain threshold, or take only the top percentage, or weight it based on the average.\n",
    "\n",
    "# There are many valid strategies. You will continue to traverse a path, until the score starts to decrease, or you notice the similarity score drops significantly (compared to the previous edges). \n",
    "# Overall try a few different approaches, and choose one that gives you the best overall F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82424f2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'[' was never closed (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[16], line 2\u001b[0m\n    graph = load_graph()\u001b[0m\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 83\u001b[1;36m in \u001b[1;35mload_graph\u001b[1;36m\n\u001b[1;33m    line = eval(line[:-1])\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>:1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ['m.01_6q0k', 'location.postal_code.country', 'm.09c7w0'\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m '[' was never closed\n"
     ]
    }
   ],
   "source": [
    "#query_df = load_query_df('annotations.txt')\n",
    "graph = load_graph()\n",
    "node_lookup = load_node_label_lookup('node_lookup.json')\n",
    "get_rel_score_word2vecbase\n",
    "\n",
    "# Relation must start with ns:\n",
    "word2vec_model.wv.index_to_key\n",
    "\n",
    "# if rel not in word2vec_model.wv:\n",
    "#         return 0.0\n",
    "# Instead of comparing embeddings we are comparing strings with keys after this change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1bdd1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastparquet\n",
      "  Downloading fastparquet-2024.11.0-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\koola\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\koola\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastparquet) (1.26.4)\n",
      "Collecting cramjam>=2.3 (from fastparquet)\n",
      "  Downloading cramjam-2.10.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting fsspec (from fastparquet)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\koola\\appdata\\roaming\\python\\python312\\site-packages (from fastparquet) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\koola\\appdata\\roaming\\python\\python312\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\koola\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\koola\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\koola\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.16.0)\n",
      "Downloading fastparquet-2024.11.0-cp312-cp312-win_amd64.whl (673 kB)\n",
      "   ---------------------------------------- 0.0/673.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 673.3/673.3 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading cramjam-2.10.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------------------- -- 1.6/1.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 2.5 MB/s eta 0:00:00\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Installing collected packages: fsspec, cramjam, fastparquet\n",
      "Successfully installed cramjam-2.10.0 fastparquet-2024.11.0 fsspec-2025.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f46ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
